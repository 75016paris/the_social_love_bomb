import tweepy
import time
import logging
from datetime import datetime, timedelta
from pathlib import Path
import json
from typing import Optional, Dict, Any, Tuple, List
from database.models import Bot, Tweet
from database.db_manager import DatabaseManager
from tweet_generator import generate_spoof, generate_reply

logger = logging.getLogger(__name__)

class BotManager:
    def __init__(self):
        self.db = DatabaseManager()
        self.project_root = Path(__file__).resolve().parent.parent
        self.cache_dir = self.project_root / "bots_twitter_cache"
        self.cache_dir.mkdir(exist_ok=True)

        self.rate_limited: Dict[str, float] = {}  # bot_name -> reset_time
        self.rate_limited_endpoints: Dict[str, Dict[str, float]] = {}  # bot_name -> endpoint -> reset_time
        self.active_bots: List[Bot] = []
        self.current_index = 0

        logger.info(f"Using cache directory: {self.cache_dir}")


    def get_user_id(self, bot: Bot, client) -> str:
        """Retrieve the user ID for the bot if not already cached in the database."""
        
        if bot.user_id:
            logger.info(f"User ID for bot {bot.name} already in the database: {bot.user_id}")
            return bot.user_id  # Return the cached user ID from the database
        
        logger.info(f"User ID not found in database for bot {bot.name}, retrieving from API...")

        try:
            # Retrieve user information from Twitter API if user_id is not cached
            response = self.execute_request(
                bot_name=bot.name,
                request_func=client.get_me,
                endpoint="get_me"
            )

            if response and hasattr(response, 'data') and 'id' in response.data:
                user_id = response.data['id']
                logger.info(f"Retrieved user ID for {bot.name}: {user_id}")

                # Store the user ID in the database for future use
                success = self.db.update_user_id(bot.id, user_id)  # Update the DB with the new user ID
                if success:
                    bot.user_id = user_id  # Update the bot's user_id attribute locally
                    logger.info(f"User ID for {bot.name} saved successfully in the database.")
                    return user_id
                else:
                    logger.error(f"Failed to save user ID for {bot.name}.")
                    return None

            else:
                logger.error(f"Failed to retrieve user ID for bot {bot.name} from API.")
                return None

        except Exception as e:
            logger.error(f"Error retrieving user ID for bot {bot.name}: {e}")
            return None

    # ---------------- Cache Management ----------------
    def _get_cache_file(self, bot_name: str, request_type: str) -> Path:
        return self.cache_dir / f"{bot_name}_{request_type}_cache.json"

    def get_cache(self, bot_name: str, request_type: str) -> Optional[Dict]:
        """Retrieve cached data if available and not expired."""
        cache_file = self._get_cache_file(bot_name, request_type)
        if not cache_file.exists():
            return None
        try:
            with cache_file.open('r') as f:
                cache_data = json.load(f)
            if time.time() > cache_data.get('expires_at', 0):
                logger.info(f"Cache expired for {bot_name} {request_type}.")
                return None
            return cache_data.get('data')
        except json.JSONDecodeError as e:
            logger.error(f"Cache file corrupted: {cache_file}. Error: {e}")
            return None
        except Exception as e:
            logger.error(f"Error reading cache: {e}")
            return None

    def set_cache(self, bot_name: str, request_type: str, data: Any, ttl: int = 300):
        """Save data to cache with a specified time-to-live."""
        try:
            cache_file = self._get_cache_file(bot_name, request_type)
            cache_data = {
                'data': data,
                'expires_at': time.time() + ttl,
                'cached_at': time.time()
            }
            with cache_file.open('w') as f:
                json.dump(cache_data, f)
        except Exception as e:
            logger.error(f"Error writing cache: {e}")

    def clean_expired_cache(self):
        """Remove expired cache files."""
        for cache_file in self.cache_dir.glob("*_cache.json"):
            try:
                with cache_file.open('r') as f:
                    cache_data = json.load(f)
                if time.time() > cache_data.get('expires_at', 0):
                    cache_file.unlink()
            except Exception:
                pass

    # ---------------- Rate Limit Management ----------------
    def mark_rate_limited(self, bot_name: str, endpoint: str, reset_time: float):
        """Mark a bot's specific endpoint as rate-limited."""
        if bot_name not in self.rate_limited_endpoints:
            self.rate_limited_endpoints[bot_name] = {}
        self.rate_limited_endpoints[bot_name][endpoint] = reset_time
        logger.warning(f"Bot {bot_name} rate limited on {endpoint} until {datetime.fromtimestamp(reset_time)}")

    def is_rate_limited(self, bot_name: str, endpoint: str) -> bool:
        """Check if a bot's specific endpoint is currently rate-limited."""
        if bot_name in self.rate_limited_endpoints and endpoint in self.rate_limited_endpoints[bot_name]:
            reset_time = self.rate_limited_endpoints[bot_name][endpoint]
            if time.time() < reset_time:
                return True
            del self.rate_limited_endpoints[bot_name][endpoint]  # Remove expired limit
        return False

    def is_bot_rate_limited(self, bot_name: str) -> bool:
        """Check if a bot is globally rate-limited."""
        if bot_name in self.rate_limited:
            if time.time() < self.rate_limited[bot_name]:
                return True
            del self.rate_limited[bot_name]  # Remove expired limit
        return False

    def clean_expired_limits(self):
        """Clear expired endpoint-specific rate limits."""
        current_time = time.time()
        for bot_name in list(self.rate_limited_endpoints.keys()):
            self.rate_limited_endpoints[bot_name] = {
                endpoint: reset_time
                for endpoint, reset_time in self.rate_limited_endpoints[bot_name].items()
                if current_time < reset_time
            }
            if not self.rate_limited_endpoints[bot_name]:
                del self.rate_limited_endpoints[bot_name]  # Remove empty entries

    def clean_global_expired_limits(self):
        """Clear expired global rate limits."""
        current_time = time.time()
        self.rate_limited = {k: v for k, v in self.rate_limited.items() if current_time < v}

    # ---------------- Twitter API Management ----------------
    def _enforce_rate_limit(self, bot_name: str):
        """Wait until rate limit is lifted for a bot."""
        if self.is_rate_limited(bot_name):
            reset_time = self.rate_limited[bot_name]
            wait_time = max(0, reset_time - time.time())
            if wait_time > 0:
                logger.info(f"Waiting {wait_time:.2f}s for bot {bot_name} rate limit reset.")
                time.sleep(wait_time)

    def _handle_rate_limit_error(self, bot_name: str, endpoint: str, error: tweepy.errors.TooManyRequests) -> int:
        """
        Handle rate limit errors and return the reset time.
        """
        logger.error(f"Rate limit error for bot {bot_name} at endpoint {endpoint}: {error}")
        reset_time = int(time.time() + 900)  # Default fallback reset time

        if hasattr(error, "response") and hasattr(error.response, "headers"):
            headers = error.response.headers
            reset_time = int(headers.get("x-rate-limit-reset", reset_time))
            self.mark_rate_limited(bot_name, endpoint, reset_time)
            logger.info(f"Rate limited until {datetime.fromtimestamp(reset_time)} for bot {bot_name} at endpoint {endpoint}.")

        return reset_time


    def execute_request(self, bot_name: str, request_func: Any, endpoint: str, **kwargs) -> Optional[Any]:
        """
        Execute a Twitter API request with retries and rate limit handling.
        """
        retries = 3  # Number of retry attempts
        for attempt in range(retries):
            try:
                # Log request details
                logger.info(f"ðŸ” Attempting request for {bot_name} - Endpoint: {endpoint}")
                logger.debug(f"ðŸ”‘ Parameters: {kwargs}")

                # Check rate limit for this endpoint
                if self.is_rate_limited(bot_name, endpoint):
                    reset_time = self.rate_limited_endpoints[bot_name][endpoint]
                    wait_time = max(0, reset_time - time.time())
                    logger.info(f"â³ Waiting {wait_time:.2f}s for bot {bot_name}, endpoint {endpoint} rate limit reset.")
                    time.sleep(wait_time)

                # Make the API request
                response = request_func(**kwargs)

                # Log successful response
                logger.info(f"ðŸ“¥ Response received for {bot_name} - Endpoint: {endpoint}")

                # Process headers for rate limits
                if hasattr(response, "headers") and response.headers:
                    headers = response.headers
                    self._process_rate_limit_headers(bot_name, endpoint, headers)

                return response

            except tweepy.errors.TooManyRequests as e:
                # Handle rate limit error
                reset_time = self._handle_rate_limit_error(bot_name, endpoint, e)
                wait_time = max(0, reset_time - time.time())
                logger.warning(f"âš ï¸ Rate limit hit for {bot_name}, endpoint {endpoint}. Retrying in {wait_time:.2f}s.")
                time.sleep(wait_time)

            except Exception as e:
                # Log other exceptions
                logger.error(f"âŒ Error on attempt {attempt + 1}/{retries} for {bot_name}, endpoint {endpoint}: {e}")
                if hasattr(e, 'response') and hasattr(e.response, 'status_code'):
                    logger.error(f"   Status Code: {e.response.status_code}")
                    logger.error(f"   Response Body: {e.response.text}")
                time.sleep(2)

        # If all retries fail
        logger.error(f"âŒ All retries failed for bot {bot_name}, endpoint {endpoint}. Giving up.")
        return None

    # Helper functions

    def _process_rate_limit_headers(self, bot_name: str, endpoint: str, headers: Dict[str, str]):
        """
        Process rate limit headers and update rate-limited status.
        """

    # ---------------- Bot Processing ----------------
    def get_api_client(self, bot: Bot):
        """Create a Twitter API client for a bot."""
        from twitter_poster import create_api
        return create_api({
            'api_key': bot.api_key,
            'api_secret': bot.api_secret,
            'access_token': bot.access_token,
            'access_token_secret': bot.access_token_secret,
            'bearer_token': bot.bearer_token
        })

    def process_article(self, bot: Bot, headline: str, description: str) -> bool:
        """Process and tweet an article."""
        if self.db.is_title_tweeted(headline):
            logger.info(f"Article already processed: {headline[:50]}...")
            return False

        client = self.get_api_client(bot)
        if not client:
            logger.error(f"Failed to create API client for bot {bot.name}.")
            return False

        # Generate tweet content
        tweet_text = generate_spoof(headline=headline, bot_identity=bot.identity)
        if not tweet_text:
            logger.error(f"Failed to generate tweet for {bot.name}")
            return False

        # Post the tweet
        logger.info(f"Attempting to tweet for {bot.name}: {tweet_text[:50]}...")
        response = self.execute_request(
            bot_name=bot.name,
            request_func=client.create_tweet,
            endpoint="post_tweet",
            text=tweet_text[:280]
        )

        if response and hasattr(response, 'data') and 'id' in response.data:
            logger.info(f"Tweet successfully posted: {response.data['id']}")
            self.db.save_tweet(
                bot_id=bot.id,
                original_title=headline,
                original_description=description,
                generated_tweet=tweet_text,
                tweet_id=response.data['id'],
                success=True
            )
            return True
        else:
            logger.error(f"Failed to post tweet. Response: {response}")
            return False


    def process_mentions(self, bot: Bot) -> bool:
        """Process replies to the bot's last tweets."""
        client = self.get_api_client(bot)
        if not client:
            logger.error(f"Failed to create API client for bot {bot.name}.")
            return False

        try:
            # RÃ©cupÃ©rer les derniers tweets du bot
            recent_tweets = self.execute_request(
                bot_name=bot.name,
                request_func=client.get_users_tweets,
                endpoint="get_users_tweets",
                id=bot.user_id,
                max_results=5
            )

            # VÃ©rification COMPLÃˆTE des tweets rÃ©cents
            if not recent_tweets:
                logger.info(f"No tweets returned for bot {bot.name}")
                return False
                
            if not hasattr(recent_tweets, 'data'):
                logger.info(f"No data attribute in tweets for bot {bot.name}")
                return False
                
            if not recent_tweets.data:
                logger.info(f"Empty data in tweets for bot {bot.name}")
                return False

            # Check each tweet for replies
            for tweet in recent_tweets.data:
                replies = self.execute_request(
                    bot_name=bot.name,
                    request_func=client.search_recent_tweets,
                    endpoint="search_recent_tweets",
                    query=f"conversation_id:{tweet.id}",
                    max_results=10
                )

                if not replies or not hasattr(replies, 'data') or not replies.data:
                    continue

                if not replies or not hasattr(replies, 'data'):
                    continue

                for reply in replies.data:
                    # Ensure it's a valid reply and generate a response
                    if reply.author_id != bot.user_id:
                        response_text = generate_reply(
                            headline=tweet.text,
                            bot_identity=bot.identity,
                            reply_text=reply.text
                        )
                        if response_text:
                            success = self.execute_request(
                                bot_name=bot.name,
                                request_func=client.create_tweet,
                                endpoint="reply_tweet",
                                text=response_text,
                                in_reply_to_tweet_id=reply.id
                            )
                            if success:
                                logger.info(f"Reply posted for bot {bot.name} to reply ID {reply.id}")
                                return True
                            
            
        except Exception as e:
            logger.error(f"Error processing mentions for bot {bot.name}: {e}", exc_info=True)
            return False
        return False


    def fetch_replies_in_batches(self, client, bot_name: str, tweet_id: str, batch_size: int = 10):
        """Fetch replies to a specific tweet in batches."""
        next_token = None
        all_replies = []

        try:
            while True:
                # Perform a batched query for replies
                params = {
                    "query": f"conversation_id:{tweet_id}",
                    "max_results": batch_size,
                    "next_token": next_token
                }
                response = self.execute_request(
                    bot_name=bot_name,
                    request_func=client.search_recent_tweets,
                    endpoint="search_recent_tweets",
                    **params
                )

                if not response or not hasattr(response, 'data') or len(response.data) == 0:
                    break

                all_replies.extend(response.data)

                # Check for pagination token
                next_token = response.meta.get("next_token", None)
                if not next_token:
                    break

        except Exception as e:
            logger.error(f"Error fetching replies in batches for tweet ID {tweet_id}: {e}")

        return all_replies

    def format_time_remaining(self, seconds: int) -> str:
        """Format seconds into a readable time string."""
        days, remainder = divmod(seconds, 86400)
        hours, remainder = divmod(remainder, 3600)
        minutes, seconds = divmod(remainder, 60)
        
        parts = []
        if days > 0:
            parts.append(f"{days}d")
        if hours > 0:
            parts.append(f"{hours}h")
        if minutes > 0:
            parts.append(f"{minutes}m")
        if seconds > 0 or not parts:
            parts.append(f"{seconds}s")
            
        return " ".join(parts)